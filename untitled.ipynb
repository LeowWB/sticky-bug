{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#6B49F5> A Simple Implementation of FedAvg with PyTorch on IIDÂ Data </font> \n",
    "Please see https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "search for other TODOs. if you're not gonna deal w them, remove them so the code looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUGGING_RUN_FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\documents\\github\\sticky-bug\\env\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "d:\\documents\\github\\sticky-bug\\env\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\Documents\\GitHub\\sticky-bug\\env\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,) (10000, 784) (10000,) (10000, 784) (10000,)\n",
      "0 >> train: 4932 , valid: 991 , test: 980 , total: 6903\n",
      "1 >> train: 5678 , valid: 1064 , test: 1135 , total: 7877\n",
      "2 >> train: 4968 , valid: 990 , test: 1032 , total: 6990\n",
      "3 >> train: 5101 , valid: 1030 , test: 1010 , total: 7141\n",
      "4 >> train: 4859 , valid: 983 , test: 982 , total: 6824\n",
      "5 >> train: 4506 , valid: 915 , test: 892 , total: 6313\n",
      "6 >> train: 4951 , valid: 967 , test: 958 , total: 6876\n",
      "7 >> train: 5175 , valid: 1090 , test: 1028 , total: 7293\n",
      "8 >> train: 4842 , valid: 1009 , test: 974 , total: 6825\n",
      "9 >> train: 4988 , valid: 961 , test: 1009 , total: 6958\n",
      "y_train_total= 50000\n",
      "y_valid_total= 10000\n",
      "y_test_total= 10000\n",
      "total= 70000\n"
     ]
    }
   ],
   "source": [
    "'''read dataset'''\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "# PATH = os.join.path(p1, p2)\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "        \n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "# Let's see the dataset size\n",
    "print(x_train.shape, y_train.shape , x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# Let's check how many of each tag are.\n",
    "y_train_total=0\n",
    "y_valid_total=0\n",
    "y_test_total=0\n",
    "total=0\n",
    "for i in range(10):\n",
    "    print(i,\">> train:\", sum(y_train==i), \", valid:\", sum(y_valid==i), \n",
    "          \", test:\", sum(y_test==i), \", total:\", sum(y_train==i)+sum(y_valid==i)+sum(y_test==i) )\n",
    "    y_train_total=y_train_total + sum(y_train==i)\n",
    "    y_valid_total=y_valid_total + sum(y_valid==i)\n",
    "    y_test_total=y_test_total + sum(y_test==i)\n",
    "    total=total+sum(y_train==i)+sum(y_valid==i)+sum(y_test==i)\n",
    "    \n",
    "print(\"y_train_total=\", y_train_total) \n",
    "print(\"y_valid_total=\", y_valid_total) \n",
    "print(\"y_test_total=\", y_test_total)\n",
    "print(\"total=\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, axes = pyplot.subplots(8,8,figsize=(8,8))\n",
    "# for i in range(8):\n",
    "#     for j in range(8):\n",
    "#         num_index = np.random.randint(len(x_train))\n",
    "#         axes[i,j].imshow(x_train[num_index].reshape((28,28)), cmap=\"gray\")\n",
    "#         axes[i,j].axis(\"off\")\n",
    "# pyplot.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In order to distribute the data to the nodes as IID, an equal number of data from each label must be taken. \n",
    "This function groups them and shuffles the order within itself. \n",
    "Please be aware that, what shuffled here is the indexes of the data, we will use them when retrieving the data in the future\n",
    "But these indexes need to be reset to avoid key errors. \n",
    "Therefore, a new column has been defined and shuffled indexes are kept there.\n",
    "\n",
    "returns dict. dict's keys are of the form 'label0', values are pandas dataframes which indicate the indices in the dataset with that label.\n",
    "'''\n",
    "def split_and_shuffle_labels(y_data, seed, amount):\n",
    "    y_data = y_data.clone().cpu()\n",
    "    y_data=pd.DataFrame(y_data,columns=[\"labels\"])\n",
    "    y_data[\"i\"]=np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    for i in range(10):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i]\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=label_info[0:amount]\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function divides the indexes in each node with an equal number of each label. \n",
    "(Here the indexes are still distributed, not data)\n",
    "\n",
    "returns dict, keys are 'sample0' up to number of samples. values are pandas dataframes, each of which contains a buncha indices\n",
    "of the dataset. they're grouped such that each dataframe (sample0, sample1, etc) has the same number of each label present.\n",
    "'''\n",
    "def get_iid_subsamples_indices(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for j in range(10):\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict\n",
    "\n",
    "'''\n",
    "same as above but clearly imbalanced. \n",
    "if done w number_of_samples=3 then party 0 has [0,0], party 1 has [1,3], party 2 has [4,9]\n",
    "'''\n",
    "def get_iid_subsamples_indices_imbalanced(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for k in range(10):\n",
    "            if i==0:\n",
    "                j = k % 2\n",
    "            elif i==1:\n",
    "                j = k % 5\n",
    "            else:\n",
    "                j = k\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' distributes x and y data to nodes in dictionary.'''\n",
    "def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"]))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Classification Model </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Functions for Federated Averaging </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function creates a model, optimizer and loss function for each node.'''\n",
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    return create_model_optimizer_criterion_dict_for_these_parties(range(number_of_samples))\n",
    "\n",
    "def create_model_optimizer_criterion_dict_for_these_parties(parties):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in parties:\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_info.to(device)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' takes the average of the weights in individual nodes.'''\n",
    "\n",
    "def get_averaged_weights(model_dict, number_of_samples, name_of_models):\n",
    "    return get_averaged_weights_for_these_parties(model_dict, range(number_of_samples), name_of_models)\n",
    "\n",
    "def get_averaged_weights_for_these_parties(model_dict, parties, name_of_models):\n",
    "\n",
    "    first_model = model_dict[list(model_dict.keys())[0]]\n",
    "    \n",
    "    fc1_mean_weight = torch.zeros(size=first_model.fc1.weight.shape).to(device)\n",
    "    fc1_mean_bias = torch.zeros(size=first_model.fc1.bias.shape).to(device)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=first_model.fc2.weight.shape).to(device)\n",
    "    fc2_mean_bias = torch.zeros(size=first_model.fc2.bias.shape).to(device)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=first_model.fc3.weight.shape).to(device)\n",
    "    fc3_mean_bias = torch.zeros(size=first_model.fc3.bias.shape).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in parties:\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/len(parties)\n",
    "        fc1_mean_bias = fc1_mean_bias/ len(parties)\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/len(parties)\n",
    "        fc2_mean_bias = fc2_mean_bias/ len(parties)\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/len(parties)\n",
    "        fc3_mean_bias = fc3_mean_bias/ len(parties)\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sends the averaged weights of individual nodes to the main model and sets them as the new weights of the main model.'''\n",
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models):\n",
    "    return set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model, model_dict, range(number_of_samples), name_of_models)\n",
    "    \n",
    "def set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights_for_these_parties(model_dict, parties, name_of_models)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model\n",
    "\n",
    "\n",
    "''' sends the parameters of the main model to the nodes.'''\n",
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models):\n",
    "    return send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, range(number_of_samples), name_of_models)\n",
    "\n",
    "def send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models):\n",
    "    with torch.no_grad():\n",
    "        for i in parties:\n",
    "\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compares the accuracy of the main model and the local model running on each node.'''\n",
    "def compare_local_and_merged_model_performance(number_of_samples):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_samples,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_samples):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trains individual local models in nodes.'''\n",
    "def start_train_end_node_process_print_some(number_of_samples, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "                                            y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                            y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                            name_of_criterions, optimizer_dict, name_of_optimizers):\n",
    "    return start_train_end_node_process_print_some_for_these_parties(range(number_of_samples), print_amount, \n",
    "                                                                     x_train_dict, name_of_x_train_sets,\n",
    "                                                                     y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                     y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                     name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "\n",
    "def start_train_end_node_process_print_some_for_these_parties(parties, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "                                            y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                            y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                            name_of_criterions, optimizer_dict, name_of_optimizers):\n",
    "    for i in parties: \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid,x_test, y_test = map(\n",
    "    lambda dset: torch.tensor(dset, device=device), \n",
    "    (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    ")\n",
    "\n",
    "centralized_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "number_of_samples=3 # also sets the number of parties for fedavg\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "\n",
    "train_amount = 100 if DEBUGGING_RUN_FAST else 4500\n",
    "valid_amount=900\n",
    "test_amount=900\n",
    "print_amount=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> Normal (nonfederated) learning </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# centralized_model = Net2nn()\n",
    "# centralized_model.to(device)\n",
    "# centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# centralized_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_ds = TensorDataset(x_train, y_train)\n",
    "# train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# valid_ds = TensorDataset(x_valid, y_valid)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# test_ds = TensorDataset(x_test, y_test)\n",
    "# test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# print(\"------ Centralized Model ------\")\n",
    "# for epoch in range(numEpoch):\n",
    "#     central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "#     central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "\n",
    "#     print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "\n",
    "# print(\"------ Training finished ------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> FedAvg </span>\n",
    "\n",
    "**Data is distributed to nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "sample_dict_train=get_iid_subsamples_indices_imbalanced(label_dict=label_dict_train, number_of_samples=number_of_samples, amount=train_amount)\n",
    "x_train_dict, y_train_dict = create_iid_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")\n",
    "\n",
    "\n",
    "label_dict_valid = split_and_shuffle_labels(y_data=y_valid, seed=1, amount=train_amount) \n",
    "sample_dict_valid = get_iid_subsamples_indices_imbalanced(label_dict=label_dict_valid, number_of_samples=number_of_samples, amount=valid_amount)\n",
    "x_valid_dict, y_valid_dict = create_iid_subsamples(sample_dict=sample_dict_valid, x_data=x_valid, y_data=y_valid, x_name=\"x_valid\", y_name=\"y_valid\")\n",
    "\n",
    "label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=1, amount=test_amount) \n",
    "sample_dict_test = get_iid_subsamples_indices_imbalanced(label_dict=label_dict_test, number_of_samples=number_of_samples, amount=test_amount)\n",
    "x_test_dict, y_test_dict = create_iid_subsamples(sample_dict=sample_dict_test, x_data=x_test, y_data=y_test, x_name=\"x_test\", y_name=\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "# print(x_valid_dict[\"x_valid1\"].shape, y_valid_dict[\"y_valid1\"].shape) \n",
    "# print(x_test_dict[\"x_test1\"].shape, y_test_dict[\"y_test1\"].shape)\n",
    "\n",
    "# num_index = np.random.randint(test_amount/number_of_samples*10)\n",
    "# pyplot.imshow(x_test_dict[\"x_test0\"].clone().cpu()[num_index].reshape((28,28)), cmap=\"gray\")\n",
    "# print(y_test_dict[\"y_test0\"].clone().cpu()[num_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main model is created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_model = Net2nn()\n",
    "# main_model.to(device)\n",
    "# main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Models,optimizers and loss functions in nodes are defined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keys of dicts are being made iterable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "name_of_x_valid_sets=list(x_valid_dict.keys())\n",
    "name_of_y_valid_sets=list(y_valid_dict.keys())\n",
    "name_of_x_test_sets=list(x_test_dict.keys())\n",
    "name_of_y_test_sets=list(y_test_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(main_model.fc2.weight[0:1,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters of main model are sent to nodes**  \n",
    "Since the parameters of the main model and parameters of all local models in the nodes are randomly initialized, all these parameters will be different from each other. For this reason, the main model sends its parameters to the nodes before the training of local models in the nodes begins. You can check the weights below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(main_model.fc2.weight[0:1,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models in the nodes are trained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_train_end_node_process_print_some(number_of_samples, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "#                                             y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "#                                             y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "#                                             name_of_criterions, optimizer_dict, name_of_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## As you can see, wieghts of local models are updated after training process\n",
    "# print(main_model.fc2.weight[0,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's compare the performance of federated main model, individual local models and centralized model  \n",
    "\n",
    "**Federated main model vs individual local models before 1st iteration (on distributed test set)**  \n",
    "Since main model is randomly initialized and no action taken on it yet, its performance is very poor. Please before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# before_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "# before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "\n",
    "# main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models) \n",
    "\n",
    "# after_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "# after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "# before_acc_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "# after_acc_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Federated main model vs centralized model before 1st iteration (on all test data)**  \n",
    "Please be aware that the centralized model gets approximately %98 accuracy on all test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "# print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "# print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single iteration, we can send the weights of the main model back to the nodes and repeat the above steps.\n",
    "Now let's check how the performance of the main model improves when we repeat the iteration 10 more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models)\n",
    "#     start_train_end_node_process_print_some(number_of_samples, -1, x_train_dict, name_of_x_train_sets,\n",
    "#                                             y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "#                                             y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "#                                             name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "#     main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models) \n",
    "#     test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "#     print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the centralized model was calculated as approximately 98%. The accuracy of the main model obtained by FedAvg method started from 85% and improved to 94%. In this case, we can say that although the main model obtained by FedAvg method was trained without seeing the data, its performance cannot be underestimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#F087F9\"> Architecture change </span>\n",
    "\n",
    "**Preparing makeshift dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKESHIFT_DSET_SIZE = 2500 if DEBUGGING_RUN_FAST else 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmm(X):\n",
    "    obs = np.array(X.cpu())\n",
    "    g_mixture = BayesianGaussianMixture(n_components=20, random_state=0, n_init=3, max_iter=5)\n",
    "    g_mixture.fit(obs)\n",
    "    return g_mixture\n",
    "\n",
    "def gmm_generate_data(gmm, num_points):\n",
    "    return torch.tensor(gmm.sample(num_points)[0], device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gmms = []\n",
    "# for i in range(number_of_samples):\n",
    "#     gmms.append(get_gmm(x_train_dict[name_of_x_train_sets[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_samps = []\n",
    "\n",
    "# for i in range(number_of_samples):\n",
    "#     samps = gmm_generate_data(gmms[i], 10)\n",
    "#     all_samps.append(samps.reshape((280,28)).cpu())\n",
    "\n",
    "# pyplot.imshow(np.concatenate(all_samps, axis=1), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # remember each party has their own makeshift dataset.\n",
    "\n",
    "# main_model.eval()\n",
    "# x_makeshift = []\n",
    "# y_makeshift = []\n",
    "\n",
    "# for i in range(number_of_samples):\n",
    "#     makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "#     makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "#     x_makeshift.append(makeshift_dset_features)\n",
    "#     y_makeshift.append(makeshift_dset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**functions for changing architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i'm assuming that layers_before_train are all regular Linear layers, while layers_to_train and layers_after_train are all single layer models.\n",
    "so i'll apply relu after the layers_before_train. \n",
    "'''\n",
    "def train_some_layers(layers_before_train, layers_to_train, layers_after_train, party_index, x_makeshift, y_makeshift):\n",
    "    \n",
    "    params_to_train = []\n",
    "    \n",
    "    for layer in layers_before_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    for layer in layers_to_train:\n",
    "        layer.train()\n",
    "        params_to_train += list(layer.parameters())\n",
    "        \n",
    "    for layer in layers_after_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(params_to_train, lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    train_ds = TensorDataset(x_makeshift[party_index], y_makeshift[party_index])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for data, target in train_dl:\n",
    "        output = data\n",
    "        \n",
    "        for layer in layers_before_train:\n",
    "            output = layer(output)\n",
    "            output = F.relu(output)\n",
    "        for layer in layers_to_train:\n",
    "            output = layer(output)\n",
    "        for layer in layers_after_train:\n",
    "            output = layer(output)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**change architecture (just as a test run)** \n",
    "\n",
    "these are proofs-of-concept to verify that our customization process works. we won't be using the customized models, but we're making customized models anyway to show that, yeah, my methodology does its job well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# each layer is a new model.\n",
    "\n",
    "# original was        784 -> 200 -> 200 -> 10\n",
    "# p1 change to        784 -> 150 -> 100 -> 10\n",
    "\n",
    "# this will show that we can remove neurons.\n",
    "# '''\n",
    "\n",
    "# class Nn_p1_fc1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc1, self).__init__()\n",
    "#         self.fc1=nn.Linear(784,150)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc1(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc2a(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc2a, self).__init__()\n",
    "#         self.fc2=nn.Linear(200,100)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc2(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc2b(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc2b, self).__init__()\n",
    "#         self.fc2=nn.Linear(150,100)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc2(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc3, self).__init__()\n",
    "#         self.fc3=nn.Linear(100,10)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x= self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "# p1_fc1 = Nn_p1_fc1()\n",
    "# p1_fc2a = Nn_p1_fc2a()\n",
    "# p1_fc2b = Nn_p1_fc2b()\n",
    "# p1_fc3 = Nn_p1_fc3()\n",
    "\n",
    "# p1_fc1.to(device)\n",
    "# p1_fc2a.to(device)\n",
    "# p1_fc2b.to(device)\n",
    "# p1_fc3.to(device)\n",
    "\n",
    "# # train fc2a (200->100), fc3 (100->10)\n",
    "# # after this the architecture will be 784 -> 200 -> 100 -> 10\n",
    "# train_some_layers(\n",
    "#     [main_model.fc1],\n",
    "#     [p1_fc2a, p1_fc3],\n",
    "#     [],\n",
    "#     0,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# # train fc1 (784->400), fc2b (400->100)\n",
    "# # after this the architecture will be 784 -> 150 -> 100 -> 10\n",
    "# train_some_layers(\n",
    "#     [],\n",
    "#     [p1_fc1, p1_fc2b],\n",
    "#     [p1_fc3],\n",
    "#     0,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# p1_model = nn.Sequential(p1_fc1,p1_fc2b,p1_fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# each layer is a new model.\n",
    "\n",
    "# original was        784 -> 200 -> 200 -> 10\n",
    "# p2 change to        784 -> 200 -> 10\n",
    "\n",
    "# this will show that we can remove entire layers\n",
    "# '''\n",
    "\n",
    "# class Nn_p2_fc1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p2_fc1, self).__init__()\n",
    "#         self.fc1=nn.Linear(784,200)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc1(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p2_fc2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p2_fc2, self).__init__()\n",
    "#         self.fc2=nn.Linear(200,10)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "    \n",
    "# p2_fc1 = Nn_p2_fc1()\n",
    "# p2_fc2 = Nn_p2_fc2()\n",
    "# p2_fc1.to(device)\n",
    "# p2_fc2.to(device)\n",
    "\n",
    "# train_some_layers(\n",
    "#     [],\n",
    "#     [p2_fc1, p2_fc2],\n",
    "#     [],\n",
    "#     1,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# p2_model = nn.Sequential(p2_fc1, p2_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test the customized models against the central post-fusion model, on the parties' own test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_model.eval()\n",
    "# p1_model.eval()\n",
    "# p2_model.eval()\n",
    "\n",
    "# p1_test_ds = TensorDataset(x_test_dict['x_test0'], y_test_dict['y_test0'])\n",
    "# p1_test_dl = DataLoader(p1_test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "# p2_test_ds = TensorDataset(x_test_dict['x_test1'], y_test_dict['y_test1'])\n",
    "# p2_test_dl = DataLoader(p2_test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "# # (main_model_loss, main_model_acc), (cust_model_loss, cust_model_acc)\n",
    "# print('p1', validation(main_model, p1_test_dl, centralized_criterion), validation(p1_model, p1_test_dl, centralized_criterion))\n",
    "# print('p2', validation(main_model, p2_test_dl, centralized_criterion), validation(p2_model, p2_test_dl, centralized_criterion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#F087F9\"> Shapley </span>\n",
    "\n",
    "**calculate value of a set of parties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent confounding, we will make all parties use the same post-customization architecture.\n",
    "\n",
    "class Nn_fc1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc1, self).__init__()\n",
    "        self.fc1=nn.Linear(784,150)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc2a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc2a, self).__init__()\n",
    "        self.fc2=nn.Linear(200,100)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc2b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc2b, self).__init__()\n",
    "        self.fc2=nn.Linear(150,100)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc3, self).__init__()\n",
    "        self.fc3=nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x= self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: list of parties. output: performance of the parties' models on their own datasets.\n",
    "\n",
    "def perform_whole_fedavg_and_customization_for_these_parties(parties): \n",
    "    main_model = Net2nn()\n",
    "    main_model.to(device)\n",
    "    main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "    for i in range(10):\n",
    "        model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "        start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "                                                                    y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                    y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                    name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "        main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "        # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "        # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "    gmms = [None, None, None]\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        gmms[i] = get_gmm(x_train_dict[name_of_x_train_sets[i]])\n",
    "\n",
    "    # remember each party has their own makeshift dataset.\n",
    "    x_makeshift = [None, None, None]\n",
    "    y_makeshift = [None, None, None]\n",
    "    main_model.eval()\n",
    "    \n",
    "    rv = [None, None, None]\n",
    "\n",
    "    for i in [0,1,2]:\n",
    "        makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "        makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "        x_makeshift[i] = makeshift_dset_features\n",
    "        y_makeshift[i] = makeshift_dset_labels\n",
    "\n",
    "        nn_fc1 = Nn_fc1()\n",
    "        nn_fc2a = Nn_fc2a()\n",
    "        nn_fc2b = Nn_fc2b()\n",
    "        nn_fc3 = Nn_fc3()\n",
    "        \n",
    "        nn_fc1.to(device)\n",
    "        nn_fc2a.to(device)\n",
    "        nn_fc2b.to(device)\n",
    "        nn_fc3.to(device)\n",
    "\n",
    "        train_some_layers(\n",
    "            [main_model.fc1],\n",
    "            [nn_fc2a, nn_fc3],\n",
    "            [],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        train_some_layers(\n",
    "            [],\n",
    "            [nn_fc1, nn_fc2b],\n",
    "            [nn_fc3],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "        customized_model.eval()\n",
    "        \n",
    "        # TODO testing should be done with makeshift dataset, not the real test set. fix it when/if you have the time. not a priority.\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "        validation_results = validation(customized_model, test_dl, centralized_criterion)\n",
    "        rv[i] = validation_results[1]\n",
    "        \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test to make sure it runs with no error\n",
    "# [perform_whole_fedavg_and_customization_for_these_parties([0,1,2]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([0]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([2]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([1,2])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actual shapley calculation stuff - diff label distribs (exp 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def my_sum(xs):\n",
    "#     rv = 0\n",
    "#     for x in xs:\n",
    "#         if x != None:\n",
    "#             rv += x\n",
    "#     return rv\n",
    "        \n",
    "# the_other_parties = [[1,2],[0,2],[0,1]]\n",
    "\n",
    "# grand_coalition_value = my_sum(perform_whole_fedavg_and_customization_for_these_parties([0,1,2]))\n",
    "# single_party_values = [\n",
    "#     my_sum(perform_whole_fedavg_and_customization_for_these_parties([party])) for party in [0,1,2]]\n",
    "# two_party_values_indexed_by_missing_party = [\n",
    "#     my_sum(perform_whole_fedavg_and_customization_for_these_parties(the_other_parties[party])) for party in [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iters_to_avg_over = 1 if DEBUGGING_RUN_FAST else 10\n",
    "# total_shapley_tus = [0,0,0]\n",
    "\n",
    "# for j in range(iters_to_avg_over):\n",
    "#     for i in range(3): # let's say we wanna find shapley of party 0.\n",
    "#         total_marginal_contribution = 2 * single_party_values[i] # v([0]) - v([]). accounts for 012 and 021 permutations\n",
    "#         other_party_a, other_party_b = the_other_parties[i]\n",
    "#         total_marginal_contribution += (\n",
    "#             two_party_values_indexed_by_missing_party[other_party_a] - single_party_values[other_party_b]) # v([0,1]) - v([1]). 102 permutation\n",
    "#         total_marginal_contribution += (\n",
    "#             two_party_values_indexed_by_missing_party[other_party_b] - single_party_values[other_party_a]) # v([0,2]) - v([2]). 201 permutation\n",
    "#         total_marginal_contribution += 2 * (\n",
    "#             grand_coalition_value - two_party_values_indexed_by_missing_party[i]) # v([0,1,2]) - v([1,2]). 210, 120 permutations\n",
    "#         total_shapley_tus[i] += total_marginal_contribution / 6\n",
    "\n",
    "# shapley_tu = [s/iters_to_avg_over for s in total_shapley_tus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # these are actly the shapley ntu values too.\n",
    "# print(shapley_tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.plot(['0','1','2'], shapley_tu)\n",
    "# plt.suptitle('Graph of Shapley-NTU contribution against party index')\n",
    "# plt.xlabel('Party index')\n",
    "# plt.ylabel('Shapley-NTU contribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**swapping one layer at a time beats retraining the whole model (exp 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# same as the earlier method except we don't swap layers. we retrain a whole new model from scratch.\n",
    "# '''\n",
    "# def perform_whole_fedavg_and_customization_for_these_parties_retrain_whole_model(parties): \n",
    "#     main_model = Net2nn()\n",
    "#     main_model.to(device)\n",
    "#     main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#     main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "#     for i in range(10):\n",
    "#         model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "#         start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "#                                                                     y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "#                                                                     y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "#                                                                     name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "#         main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "#         # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "#         # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "#     gmms = [None, None, None]\n",
    "    \n",
    "#     for i in [0,1,2]:\n",
    "#         gmms[i] = get_gmm(x_train_dict[name_of_x_train_sets[i]])\n",
    "\n",
    "#     # remember each party has their own makeshift dataset.\n",
    "#     x_makeshift = [None, None, None]\n",
    "#     y_makeshift = [None, None, None]\n",
    "#     main_model.eval()\n",
    "    \n",
    "#     rv = [None, None, None]\n",
    "\n",
    "#     for i in [0,1,2]:\n",
    "#         makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "#         makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "#         x_makeshift[i] = makeshift_dset_features\n",
    "#         y_makeshift[i] = makeshift_dset_labels\n",
    "\n",
    "#         nn_fc1 = Nn_fc1()\n",
    "#         nn_fc2b = Nn_fc2b()\n",
    "#         nn_fc3 = Nn_fc3()\n",
    "        \n",
    "#         nn_fc1.to(device)\n",
    "#         nn_fc2b.to(device)\n",
    "#         nn_fc3.to(device)\n",
    "\n",
    "#         train_some_layers(\n",
    "#             [],\n",
    "#             [nn_fc1, nn_fc2b, nn_fc3],\n",
    "#             [],\n",
    "#             i, x_makeshift, y_makeshift\n",
    "#         )\n",
    "\n",
    "#         customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "#         customized_model.eval()\n",
    "        \n",
    "#         # TODO testing should be done with makeshift dataset, not the real test set. fix it when/if you have the time. not a priority.\n",
    "#         test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "#         test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "#         validation_results = validation(customized_model, test_dl, centralized_criterion)\n",
    "#         rv[i] = validation_results[1]\n",
    "        \n",
    "#     return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_layer_by_layer_val_results = []\n",
    "# retrain_whole_model_val_results = []\n",
    "\n",
    "# for i in range(20):\n",
    "#     print(i,end=' ')\n",
    "#     train_layer_by_layer_val_results.append(perform_whole_fedavg_and_customization_for_these_parties([0,1,2]))\n",
    "#     retrain_whole_model_val_results.append(perform_whole_fedavg_and_customization_for_these_parties_retrain_whole_model([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train_layer_by_layer_val_results)\n",
    "# print(retrain_whole_model_val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.boxplot([\n",
    "#     list(map(lambda x: x[0], train_layer_by_layer_val_results)),\n",
    "#     list(map(lambda x: x[0], retrain_whole_model_val_results)),\n",
    "#     list(map(lambda x: x[1], train_layer_by_layer_val_results)),\n",
    "#     list(map(lambda x: x[1], retrain_whole_model_val_results)),\n",
    "#     list(map(lambda x: x[2], train_layer_by_layer_val_results)),\n",
    "#     list(map(lambda x: x[2], retrain_whole_model_val_results)),\n",
    "# ])\n",
    "\n",
    "# plt.xticks(range(1,7), ['0L','0M','1L','1M','2L','2M'])\n",
    "# plt.suptitle('Validation accuracies with different training methods')\n",
    "# plt.xlabel('Party index and training method (L=layer, M=model)')\n",
    "# plt.ylabel('Validation accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**demonstrating our early stopping \"binary search\" (exp 3)**\n",
    "\n",
    "strictly for all parties, not just a subset. cuz when we reduce the performance, we only do it at the end for the final reward model. and that model is always made with all parties' help. but that final model will only be the one pertaining to party 0 cuz their problem is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1 # how much performance difference we can tolerate\n",
    "\n",
    "# performance refers to actual validation accuracy. returns how the accuracy has changed (for the \"binary search\")\n",
    "# to get the performance within +-epsilon% of the target. \n",
    "def perform_whole_fedavg_and_customization_for_party_0_with_this_performance(performance):\n",
    "    parties = range(3)\n",
    "    main_model = Net2nn()\n",
    "    main_model.to(device)\n",
    "    main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "    for i in range(10):\n",
    "        model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "        start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "                                                                    y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                    y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                    name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "        main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "        # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "        # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "    gmm = get_gmm(x_train_dict[name_of_x_train_sets[0]])\n",
    "    main_model.eval()\n",
    "    \n",
    "    acc_changes = []\n",
    "\n",
    "    makeshift_dset_features = gmm_generate_data(gmm, MAKESHIFT_DSET_SIZE)\n",
    "    makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "    x_makeshift = makeshift_dset_features\n",
    "    y_makeshift = makeshift_dset_labels\n",
    "\n",
    "    adjusted_lr = learning_rate\n",
    "    adjusted_lr_bounds = [0, learning_rate * 2]\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        nn_fc1 = Nn_fc1()\n",
    "        nn_fc2a = Nn_fc2a()\n",
    "        nn_fc2b = Nn_fc2b()\n",
    "        nn_fc3 = Nn_fc3()\n",
    "\n",
    "        nn_fc1.to(device)\n",
    "        nn_fc2a.to(device)\n",
    "        nn_fc2b.to(device)\n",
    "        nn_fc3.to(device)\n",
    "\n",
    "        train_some_layers_lr(\n",
    "            [main_model.fc1],\n",
    "            [nn_fc2a, nn_fc3],\n",
    "            [],\n",
    "            0, x_makeshift, y_makeshift,\n",
    "            adjusted_lr\n",
    "        )\n",
    "\n",
    "        train_some_layers_lr(\n",
    "            [],\n",
    "            [nn_fc1, nn_fc2b],\n",
    "            [nn_fc3],\n",
    "            0, x_makeshift, y_makeshift,\n",
    "            adjusted_lr\n",
    "        )\n",
    "\n",
    "        customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "        customized_model.eval()\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[0]], y_test_dict[name_of_y_test_sets[0]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "        validation_acc = validation(customized_model, test_dl, centralized_criterion)[1]\n",
    "        acc_changes.append(validation_acc)\n",
    "\n",
    "        if abs(validation_acc - performance) < (EPSILON/100):\n",
    "            break\n",
    "        elif validation_acc > performance:\n",
    "            adjusted_lr_bounds[1] = adjusted_lr\n",
    "            adjusted_lr = sum(adjusted_lr_bounds) / 2\n",
    "        else:\n",
    "            adjusted_lr_bounds[0] = adjusted_lr\n",
    "            adjusted_lr = sum(adjusted_lr_bounds) / 2\n",
    "\n",
    "    return acc_changes\n",
    "\n",
    "def train_some_layers_lr(layers_before_train, layers_to_train, layers_after_train, party_index, x_makeshift, y_makeshift, lr):\n",
    "    \n",
    "    params_to_train = []\n",
    "    \n",
    "    for layer in layers_before_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    for layer in layers_to_train:\n",
    "        layer.train()\n",
    "        params_to_train += list(layer.parameters())\n",
    "        \n",
    "    for layer in layers_after_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(params_to_train, lr=lr, momentum=momentum)\n",
    "    \n",
    "    train_ds = TensorDataset(x_makeshift, y_makeshift)\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for data, target in train_dl:\n",
    "        output = data\n",
    "        \n",
    "        for layer in layers_before_train:\n",
    "            output = layer(output)\n",
    "            output = F.relu(output)\n",
    "        for layer in layers_to_train:\n",
    "            output = layer(output)\n",
    "        for layer in layers_after_train:\n",
    "            output = layer(output)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(5,100,5):\n",
    "    print(i)\n",
    "    next_res = []\n",
    "    results.append(next_res)\n",
    "    for i in range(10):\n",
    "        next_res.append(len(perform_whole_fedavg_and_customization_for_party_0_with_this_performance(i/100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 9, 43, 13, 28, 14, 83, 81, 12, 18],\n",
       " [9, 23, 26, 25, 20, 46, 65, 35, 16, 101],\n",
       " [11, 12, 21, 21, 21, 78, 9, 11, 64, 35],\n",
       " [11, 8, 23, 15, 68, 38, 169, 13, 13, 27],\n",
       " [11, 15, 16, 10, 16, 144, 16, 72, 50, 137],\n",
       " [9, 11, 57, 38, 8, 46, 12, 105, 31, 97],\n",
       " [10, 13, 55, 82, 10, 44, 78, 104, 16, 52],\n",
       " [16, 12, 13, 25, 64, 13, 38, 65, 75, 12],\n",
       " [12, 20, 41, 88, 15, 53, 12, 25, 52, 42],\n",
       " [9, 9, 48, 7, 39, 43, 49, 12, 7, 11],\n",
       " [8, 20, 23, 21, 33, 98, 7, 11, 71, 41],\n",
       " [10, 14, 15, 32, 52, 37, 26, 40, 16, 32],\n",
       " [12, 11, 36, 36, 72, 13, 10, 43, 201, 11],\n",
       " [8, 10, 16, 11, 23, 99, 16, 13, 30, 94],\n",
       " [10, 9, 11, 17, 39, 44, 18, 22, 12, 114],\n",
       " [9, 15, 23, 8, 69, 236, 80, 38, 79, 28],\n",
       " [8, 9, 19, 20, 131, 36, 22, 11, 169, 87],\n",
       " [9, 12, 15, 32, 23, 26, 36, 62, 79, 88],\n",
       " [8, 27, 16, 70, 13, 181, 9, 8, 21, 24]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(results)\n",
    "plt.xticks(range(1,20), range(5,100,5))\n",
    "plt.suptitle('Iterations required to achieve target performance +-1 (%)')\n",
    "plt.xlabel('Target performance (%)')\n",
    "plt.ylabel('Iterations required')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testing back to front swapping vs front to back swapping (exp 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ur task is to make this train forwards AND backwards. just get one fused model (fedavg once is enough) and do the \n",
    "changing 4wards and backwards on this same model. then return val accs. repeat.\n",
    "might wanna make more complex architecture for demonstration purposes. \n",
    "'''\n",
    "\n",
    "def perform_whole_fedavg_and_customization_for_all_parties_both_directions(parties): \n",
    "    main_model = Net2nn()\n",
    "    main_model.to(device)\n",
    "    main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "    for i in range(10):\n",
    "        model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "        start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "                                                                    y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                    y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                    name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "        main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "        # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "        # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "    gmms = [None, None, None]\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        gmms[i] = get_gmm(x_train_dict[name_of_x_train_sets[i]])\n",
    "\n",
    "    # remember each party has their own makeshift dataset.\n",
    "    x_makeshift = [None, None, None]\n",
    "    y_makeshift = [None, None, None]\n",
    "    main_model.eval()\n",
    "    \n",
    "    rv = [None, None, None]\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "        makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "        x_makeshift[i] = makeshift_dset_features\n",
    "        y_makeshift[i] = makeshift_dset_labels\n",
    "\n",
    "        nn_fc1 = Nn_fc1()\n",
    "        nn_fc2a = Nn_fc2a()\n",
    "        nn_fc2b = Nn_fc2b()\n",
    "        nn_fc3 = Nn_fc3()\n",
    "        \n",
    "        nn_fc1.to(device)\n",
    "        nn_fc2a.to(device)\n",
    "        nn_fc2b.to(device)\n",
    "        nn_fc3.to(device)\n",
    "\n",
    "        train_some_layers(\n",
    "            [main_model.fc1],\n",
    "            [nn_fc2a, nn_fc3],\n",
    "            [],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        train_some_layers(\n",
    "            [],\n",
    "            [nn_fc1, nn_fc2b],\n",
    "            [nn_fc3],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "        customized_model.eval()\n",
    "        \n",
    "        # TODO testing should be done with makeshift dataset, not the real test set. fix it when/if you have the time. not a priority.\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "        validation_results = validation(customized_model, test_dl, centralized_criterion)\n",
    "        rv[i] = validation_results[1]\n",
    "        \n",
    "    return rv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
