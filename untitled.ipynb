{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#6B49F5> A Simple Implementation of FedAvg with PyTorch on IIDÂ Data </font> \n",
    "Please see https://towardsdatascience.com/federated-learning-a-simple-implementation-of-fedavg-federated-averaging-with-pytorch-90187c9c9577 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "search for other TODOs. if you're not gonna deal w them, remove them so the code looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUGGING_RUN_FAST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\documents\\github\\sticky-bug\\env\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "d:\\documents\\github\\sticky-bug\\env\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\Documents\\GitHub\\sticky-bug\\env\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,) (10000, 784) (10000,) (10000, 784) (10000,)\n",
      "0 >> train: 4932 , valid: 991 , test: 980 , total: 6903\n",
      "1 >> train: 5678 , valid: 1064 , test: 1135 , total: 7877\n",
      "2 >> train: 4968 , valid: 990 , test: 1032 , total: 6990\n",
      "3 >> train: 5101 , valid: 1030 , test: 1010 , total: 7141\n",
      "4 >> train: 4859 , valid: 983 , test: 982 , total: 6824\n",
      "5 >> train: 4506 , valid: 915 , test: 892 , total: 6313\n",
      "6 >> train: 4951 , valid: 967 , test: 958 , total: 6876\n",
      "7 >> train: 5175 , valid: 1090 , test: 1028 , total: 7293\n",
      "8 >> train: 4842 , valid: 1009 , test: 974 , total: 6825\n",
      "9 >> train: 4988 , valid: 961 , test: 1009 , total: 6958\n",
      "y_train_total= 50000\n",
      "y_valid_total= 10000\n",
      "y_test_total= 10000\n",
      "total= 70000\n"
     ]
    }
   ],
   "source": [
    "'''read dataset'''\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "# PATH = os.join.path(p1, p2)\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)\n",
    "        \n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "# Let's see the dataset size\n",
    "print(x_train.shape, y_train.shape , x_valid.shape, y_valid.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# Let's check how many of each tag are.\n",
    "y_train_total=0\n",
    "y_valid_total=0\n",
    "y_test_total=0\n",
    "total=0\n",
    "for i in range(10):\n",
    "    print(i,\">> train:\", sum(y_train==i), \", valid:\", sum(y_valid==i), \n",
    "          \", test:\", sum(y_test==i), \", total:\", sum(y_train==i)+sum(y_valid==i)+sum(y_test==i) )\n",
    "    y_train_total=y_train_total + sum(y_train==i)\n",
    "    y_valid_total=y_valid_total + sum(y_valid==i)\n",
    "    y_test_total=y_test_total + sum(y_test==i)\n",
    "    total=total+sum(y_train==i)+sum(y_valid==i)+sum(y_test==i)\n",
    "    \n",
    "print(\"y_train_total=\", y_train_total) \n",
    "print(\"y_valid_total=\", y_valid_total) \n",
    "print(\"y_test_total=\", y_test_total)\n",
    "print(\"total=\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, axes = pyplot.subplots(8,8,figsize=(8,8))\n",
    "# for i in range(8):\n",
    "#     for j in range(8):\n",
    "#         num_index = np.random.randint(len(x_train))\n",
    "#         axes[i,j].imshow(x_train[num_index].reshape((28,28)), cmap=\"gray\")\n",
    "#         axes[i,j].axis(\"off\")\n",
    "# pyplot.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In order to distribute the data to the nodes as IID, an equal number of data from each label must be taken. \n",
    "This function groups them and shuffles the order within itself. \n",
    "Please be aware that, what shuffled here is the indexes of the data, we will use them when retrieving the data in the future\n",
    "But these indexes need to be reset to avoid key errors. \n",
    "Therefore, a new column has been defined and shuffled indexes are kept there.\n",
    "\n",
    "returns dict. dict's keys are of the form 'label0', values are pandas dataframes which indicate the indices in the dataset with that label.\n",
    "'''\n",
    "def split_and_shuffle_labels(y_data, seed, amount):\n",
    "    y_data = y_data.clone().cpu()\n",
    "    y_data=pd.DataFrame(y_data,columns=[\"labels\"])\n",
    "    y_data[\"i\"]=np.arange(len(y_data))\n",
    "    label_dict = dict()\n",
    "    for i in range(10):\n",
    "        var_name=\"label\" + str(i)\n",
    "        label_info=y_data[y_data[\"labels\"]==i]\n",
    "        np.random.seed(seed)\n",
    "        label_info=np.random.permutation(label_info)\n",
    "        label_info=label_info[0:amount]\n",
    "        label_info=pd.DataFrame(label_info, columns=[\"labels\",\"i\"])\n",
    "        label_dict.update({var_name: label_info })\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function divides the indexes in each node with an equal number of each label. \n",
    "(Here the indexes are still distributed, not data)\n",
    "\n",
    "returns dict, keys are 'sample0' up to number of samples. values are pandas dataframes, each of which contains a buncha indices\n",
    "of the dataset. they're grouped such that each dataframe (sample0, sample1, etc) has the same number of each label present.\n",
    "'''\n",
    "def get_iid_subsamples_indices(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for j in range(10):\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict\n",
    "\n",
    "'''\n",
    "same as above but clearly imbalanced. \n",
    "if done w number_of_samples=3 then party 0 has [0,0], party 1 has [1,3], party 2 has [4,9]\n",
    "'''\n",
    "def get_iid_subsamples_indices_imbalanced(label_dict, number_of_samples, amount):\n",
    "    sample_dict= dict()\n",
    "    batch_size=int(math.floor(amount/number_of_samples))\n",
    "    for i in range(number_of_samples):\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        dumb=pd.DataFrame()\n",
    "        for k in range(10):\n",
    "            if i==0:\n",
    "                j = k % 2\n",
    "            elif i==1:\n",
    "                j = k % 5\n",
    "            else:\n",
    "                j = k\n",
    "            label_name=str(\"label\")+str(j)\n",
    "            a=label_dict[label_name][i*batch_size:(i+1)*batch_size]\n",
    "            dumb=pd.concat([dumb,a], axis=0)\n",
    "        dumb.reset_index(drop=True, inplace=True)    \n",
    "        sample_dict.update({sample_name: dumb}) \n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' distributes x and y data to nodes in dictionary.'''\n",
    "def create_iid_subsamples(sample_dict, x_data, y_data, x_name, y_name):\n",
    "    x_data_dict= dict()\n",
    "    y_data_dict= dict()\n",
    "    \n",
    "    for i in range(len(sample_dict)):  ### len(sample_dict)= number of samples\n",
    "        xname= x_name+str(i)\n",
    "        yname= y_name+str(i)\n",
    "        sample_name=\"sample\"+str(i)\n",
    "        \n",
    "        indices=np.sort(np.array(sample_dict[sample_name][\"i\"]))\n",
    "        \n",
    "        x_info= x_data[indices,:]\n",
    "        x_data_dict.update({xname : x_info})\n",
    "        \n",
    "        y_info= y_data[indices]\n",
    "        y_data_dict.update({yname : y_info})\n",
    "        \n",
    "    return x_data_dict, y_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Classification Model </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2nn, self).__init__()\n",
    "        self.fc1=nn.Linear(784,200)\n",
    "        self.fc2=nn.Linear(200,200)\n",
    "        self.fc3=nn.Linear(200,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        prediction = output.argmax(dim=1, keepdim=True)\n",
    "        correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss / len(train_loader), correct/len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, target).item()\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    correct /= len(test_loader.dataset)\n",
    "\n",
    "    return (test_loss, correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "### <span style=\"background-color:#F087F9\"> Functions for Federated Averaging </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function creates a model, optimizer and loss function for each node.'''\n",
    "def create_model_optimizer_criterion_dict(number_of_samples):\n",
    "    return create_model_optimizer_criterion_dict_for_these_parties(range(number_of_samples))\n",
    "\n",
    "def create_model_optimizer_criterion_dict_for_these_parties(parties):\n",
    "    model_dict = dict()\n",
    "    optimizer_dict= dict()\n",
    "    criterion_dict = dict()\n",
    "    \n",
    "    for i in parties:\n",
    "        model_name=\"model\"+str(i)\n",
    "        model_info=Net2nn()\n",
    "        model_info.to(device)\n",
    "        model_dict.update({model_name : model_info })\n",
    "        \n",
    "        optimizer_name=\"optimizer\"+str(i)\n",
    "        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        optimizer_dict.update({optimizer_name : optimizer_info })\n",
    "        \n",
    "        criterion_name = \"criterion\"+str(i)\n",
    "        criterion_info = nn.CrossEntropyLoss()\n",
    "        criterion_dict.update({criterion_name : criterion_info})\n",
    "        \n",
    "    return model_dict, optimizer_dict, criterion_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' takes the average of the weights in individual nodes.'''\n",
    "\n",
    "def get_averaged_weights(model_dict, number_of_samples, name_of_models):\n",
    "    return get_averaged_weights_for_these_parties(model_dict, range(number_of_samples), name_of_models)\n",
    "\n",
    "def get_averaged_weights_for_these_parties(model_dict, parties, name_of_models):\n",
    "\n",
    "    first_model = model_dict[list(model_dict.keys())[0]]\n",
    "    \n",
    "    fc1_mean_weight = torch.zeros(size=first_model.fc1.weight.shape).to(device)\n",
    "    fc1_mean_bias = torch.zeros(size=first_model.fc1.bias.shape).to(device)\n",
    "    \n",
    "    fc2_mean_weight = torch.zeros(size=first_model.fc2.weight.shape).to(device)\n",
    "    fc2_mean_bias = torch.zeros(size=first_model.fc2.bias.shape).to(device)\n",
    "    \n",
    "    fc3_mean_weight = torch.zeros(size=first_model.fc3.weight.shape).to(device)\n",
    "    fc3_mean_bias = torch.zeros(size=first_model.fc3.bias.shape).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in parties:\n",
    "            fc1_mean_weight += model_dict[name_of_models[i]].fc1.weight.data.clone()\n",
    "            fc1_mean_bias += model_dict[name_of_models[i]].fc1.bias.data.clone()\n",
    "        \n",
    "            fc2_mean_weight += model_dict[name_of_models[i]].fc2.weight.data.clone()\n",
    "            fc2_mean_bias += model_dict[name_of_models[i]].fc2.bias.data.clone()\n",
    "        \n",
    "            fc3_mean_weight += model_dict[name_of_models[i]].fc3.weight.data.clone()\n",
    "            fc3_mean_bias += model_dict[name_of_models[i]].fc3.bias.data.clone()\n",
    "\n",
    "        \n",
    "        fc1_mean_weight =fc1_mean_weight/len(parties)\n",
    "        fc1_mean_bias = fc1_mean_bias/ len(parties)\n",
    "    \n",
    "        fc2_mean_weight =fc2_mean_weight/len(parties)\n",
    "        fc2_mean_bias = fc2_mean_bias/ len(parties)\n",
    "    \n",
    "        fc3_mean_weight =fc3_mean_weight/len(parties)\n",
    "        fc3_mean_bias = fc3_mean_bias/ len(parties)\n",
    "    \n",
    "    return fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sends the averaged weights of individual nodes to the main model and sets them as the new weights of the main model.'''\n",
    "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models):\n",
    "    return set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model, model_dict, range(number_of_samples), name_of_models)\n",
    "    \n",
    "def set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models):\n",
    "    fc1_mean_weight, fc1_mean_bias, fc2_mean_weight, fc2_mean_bias, fc3_mean_weight, fc3_mean_bias = get_averaged_weights_for_these_parties(model_dict, parties, name_of_models)\n",
    "    with torch.no_grad():\n",
    "        main_model.fc1.weight.data = fc1_mean_weight.data.clone()\n",
    "        main_model.fc2.weight.data = fc2_mean_weight.data.clone()\n",
    "        main_model.fc3.weight.data = fc3_mean_weight.data.clone()\n",
    "\n",
    "        main_model.fc1.bias.data = fc1_mean_bias.data.clone()\n",
    "        main_model.fc2.bias.data = fc2_mean_bias.data.clone()\n",
    "        main_model.fc3.bias.data = fc3_mean_bias.data.clone() \n",
    "    return main_model\n",
    "\n",
    "\n",
    "''' sends the parameters of the main model to the nodes.'''\n",
    "def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models):\n",
    "    return send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, range(number_of_samples), name_of_models)\n",
    "\n",
    "def send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models):\n",
    "    with torch.no_grad():\n",
    "        for i in parties:\n",
    "\n",
    "            model_dict[name_of_models[i]].fc1.weight.data =main_model.fc1.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.weight.data =main_model.fc2.weight.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.weight.data =main_model.fc3.weight.data.clone() \n",
    "            \n",
    "            model_dict[name_of_models[i]].fc1.bias.data =main_model.fc1.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc2.bias.data =main_model.fc2.bias.data.clone()\n",
    "            model_dict[name_of_models[i]].fc3.bias.data =main_model.fc3.bias.data.clone() \n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compares the accuracy of the main model and the local model running on each node.'''\n",
    "def compare_local_and_merged_model_performance(number_of_samples):\n",
    "    accuracy_table=pd.DataFrame(data=np.zeros((number_of_samples,3)), columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n",
    "    for i in range (number_of_samples):\n",
    "    \n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        individual_loss, individual_accuracy = validation(model, test_dl, criterion)\n",
    "        main_loss, main_accuracy =validation(main_model, test_dl, main_criterion )\n",
    "    \n",
    "        accuracy_table.loc[i, \"sample\"]=\"sample \"+str(i)\n",
    "        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n",
    "        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n",
    "\n",
    "    return accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''trains individual local models in nodes.'''\n",
    "def start_train_end_node_process_print_some(number_of_samples, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "                                            y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                            y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                            name_of_criterions, optimizer_dict, name_of_optimizers):\n",
    "    return start_train_end_node_process_print_some_for_these_parties(range(number_of_samples), print_amount, \n",
    "                                                                     x_train_dict, name_of_x_train_sets,\n",
    "                                                                     y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                     y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                     name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "\n",
    "def start_train_end_node_process_print_some_for_these_parties(parties, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "                                            y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                            y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                            name_of_criterions, optimizer_dict, name_of_optimizers):\n",
    "    for i in parties: \n",
    "\n",
    "        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "    \n",
    "        model=model_dict[name_of_models[i]]\n",
    "        criterion=criterion_dict[name_of_criterions[i]]\n",
    "        optimizer=optimizer_dict[name_of_optimizers[i]]\n",
    "    \n",
    "        if i<print_amount:\n",
    "            print(\"Subset\" ,i)\n",
    "            \n",
    "        for epoch in range(numEpoch):\n",
    "        \n",
    "            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer)\n",
    "            test_loss, test_accuracy = validation(model, test_dl, criterion)\n",
    "            \n",
    "            if i<print_amount:        \n",
    "                print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.5f}\".format(train_accuracy) + \" | test accuracy: {:7.5f}\".format(test_accuracy))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid,x_test, y_test = map(\n",
    "    lambda dset: torch.tensor(dset, device=device), \n",
    "    (x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    ")\n",
    "\n",
    "centralized_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "number_of_samples=3 # also sets the number of parties for fedavg\n",
    "learning_rate = 0.01\n",
    "numEpoch = 10\n",
    "batch_size = 32\n",
    "momentum = 0.9\n",
    "\n",
    "train_amount = 100 if DEBUGGING_RUN_FAST else 4500\n",
    "valid_amount=900\n",
    "test_amount=900\n",
    "print_amount=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> Normal (nonfederated) learning </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# centralized_model = Net2nn()\n",
    "# centralized_model.to(device)\n",
    "# centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "# centralized_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# train_ds = TensorDataset(x_train, y_train)\n",
    "# train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# valid_ds = TensorDataset(x_valid, y_valid)\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# test_ds = TensorDataset(x_test, y_test)\n",
    "# test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# print(\"------ Centralized Model ------\")\n",
    "# for epoch in range(numEpoch):\n",
    "#     central_train_loss, central_train_accuracy = train(centralized_model, train_dl, centralized_criterion, centralized_optimizer)\n",
    "#     central_test_loss, central_test_accuracy = validation(centralized_model, test_dl, centralized_criterion)\n",
    "\n",
    "#     print(\"epoch: {:3.0f}\".format(epoch+1) + \" | train accuracy: {:7.4f}\".format(central_train_accuracy) + \" | test accuracy: {:7.4f}\".format(central_test_accuracy))\n",
    "\n",
    "# print(\"------ Training finished ------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### <span style=\"background-color:#F087F9\"> FedAvg </span>\n",
    "\n",
    "**Data is distributed to nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_train=split_and_shuffle_labels(y_data=y_train, seed=1, amount=train_amount) \n",
    "sample_dict_train=get_iid_subsamples_indices_imbalanced(label_dict=label_dict_train, number_of_samples=number_of_samples, amount=train_amount)\n",
    "x_train_dict, y_train_dict = create_iid_subsamples(sample_dict=sample_dict_train, x_data=x_train, y_data=y_train, x_name=\"x_train\", y_name=\"y_train\")\n",
    "\n",
    "\n",
    "label_dict_valid = split_and_shuffle_labels(y_data=y_valid, seed=1, amount=train_amount) \n",
    "sample_dict_valid = get_iid_subsamples_indices_imbalanced(label_dict=label_dict_valid, number_of_samples=number_of_samples, amount=valid_amount)\n",
    "x_valid_dict, y_valid_dict = create_iid_subsamples(sample_dict=sample_dict_valid, x_data=x_valid, y_data=y_valid, x_name=\"x_valid\", y_name=\"y_valid\")\n",
    "\n",
    "label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=1, amount=test_amount) \n",
    "sample_dict_test = get_iid_subsamples_indices_imbalanced(label_dict=label_dict_test, number_of_samples=number_of_samples, amount=test_amount)\n",
    "x_test_dict, y_test_dict = create_iid_subsamples(sample_dict=sample_dict_test, x_data=x_test, y_data=y_test, x_name=\"x_test\", y_name=\"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(x_train_dict[\"x_train1\"].shape, y_train_dict[\"y_train1\"].shape)\n",
    "# print(x_valid_dict[\"x_valid1\"].shape, y_valid_dict[\"y_valid1\"].shape) \n",
    "# print(x_test_dict[\"x_test1\"].shape, y_test_dict[\"y_test1\"].shape)\n",
    "\n",
    "# num_index = np.random.randint(test_amount/number_of_samples*10)\n",
    "# pyplot.imshow(x_test_dict[\"x_test0\"].clone().cpu()[num_index].reshape((28,28)), cmap=\"gray\")\n",
    "# print(y_test_dict[\"y_test0\"].clone().cpu()[num_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main model is created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_model = Net2nn()\n",
    "# main_model.to(device)\n",
    "# main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# main_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Models,optimizers and loss functions in nodes are defined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict(number_of_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keys of dicts are being made iterable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_x_train_sets=list(x_train_dict.keys())\n",
    "name_of_y_train_sets=list(y_train_dict.keys())\n",
    "name_of_x_valid_sets=list(x_valid_dict.keys())\n",
    "name_of_y_valid_sets=list(y_valid_dict.keys())\n",
    "name_of_x_test_sets=list(x_test_dict.keys())\n",
    "name_of_y_test_sets=list(y_test_dict.keys())\n",
    "\n",
    "name_of_models=list(model_dict.keys())\n",
    "name_of_optimizers=list(optimizer_dict.keys())\n",
    "name_of_criterions=list(criterion_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(main_model.fc2.weight[0:1,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters of main model are sent to nodes**  \n",
    "Since the parameters of the main model and parameters of all local models in the nodes are randomly initialized, all these parameters will be different from each other. For this reason, the main model sends its parameters to the nodes before the training of local models in the nodes begins. You can check the weights below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(main_model.fc2.weight[0:1,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0:1,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models in the nodes are trained**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_train_end_node_process_print_some(number_of_samples, print_amount, x_train_dict, name_of_x_train_sets,\n",
    "#                                             y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "#                                             y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "#                                             name_of_criterions, optimizer_dict, name_of_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## As you can see, wieghts of local models are updated after training process\n",
    "# print(main_model.fc2.weight[0,0:5])\n",
    "# print(model_dict[\"model1\"].fc2.weight[0,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's compare the performance of federated main model, individual local models and centralized model  \n",
    "\n",
    "**Federated main model vs individual local models before 1st iteration (on distributed test set)**  \n",
    "Since main model is randomly initialized and no action taken on it yet, its performance is very poor. Please before_acc_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# before_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "# before_test_loss, before_test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "\n",
    "# main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models) \n",
    "\n",
    "# after_acc_table=compare_local_and_merged_model_performance(number_of_samples=number_of_samples)\n",
    "# after_test_loss, after_test_accuracy = validation(main_model, test_dl, main_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Federated main model vs individual local models before FedAvg first iteration\")\n",
    "# before_acc_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Federated main model vs individual local models after FedAvg first iteration\")\n",
    "# after_acc_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Federated main model vs centralized model before 1st iteration (on all test data)**  \n",
    "Please be aware that the centralized model gets approximately %98 accuracy on all test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Before 1st iteration main model accuracy on all test data: {:7.4f}\".format(before_test_accuracy))\n",
    "# print(\"After 1st iteration main model accuracy on all test data: {:7.4f}\".format(after_test_accuracy))\n",
    "# print(\"Centralized model accuracy on all test data: {:7.4f}\".format(central_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a single iteration, we can send the weights of the main model back to the nodes and repeat the above steps.\n",
    "Now let's check how the performance of the main model improves when we repeat the iteration 10 more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     model_dict=send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples, name_of_models)\n",
    "#     start_train_end_node_process_print_some(number_of_samples, -1, x_train_dict, name_of_x_train_sets,\n",
    "#                                             y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "#                                             y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "#                                             name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "#     main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,model_dict, number_of_samples, name_of_models) \n",
    "#     test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "#     print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the centralized model was calculated as approximately 98%. The accuracy of the main model obtained by FedAvg method started from 85% and improved to 94%. In this case, we can say that although the main model obtained by FedAvg method was trained without seeing the data, its performance cannot be underestimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#F087F9\"> Architecture change </span>\n",
    "\n",
    "**Preparing makeshift dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKESHIFT_DSET_SIZE = 2500 if DEBUGGING_RUN_FAST else 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gmm(X):\n",
    "    obs = np.array(X.cpu())\n",
    "    g_mixture = BayesianGaussianMixture(n_components=20, random_state=0, n_init=3, max_iter=5)\n",
    "    g_mixture.fit(obs)\n",
    "    return g_mixture\n",
    "\n",
    "def gmm_generate_data(gmm, num_points):\n",
    "    return torch.tensor(gmm.sample(num_points)[0], device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gmms = []\n",
    "# for i in range(number_of_samples):\n",
    "#     gmms.append(get_gmm(x_train_dict[name_of_x_train_sets[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_samps = []\n",
    "\n",
    "# for i in range(number_of_samples):\n",
    "#     samps = gmm_generate_data(gmms[i], 10)\n",
    "#     all_samps.append(samps.reshape((280,28)).cpu())\n",
    "\n",
    "# pyplot.imshow(np.concatenate(all_samps, axis=1), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # remember each party has their own makeshift dataset.\n",
    "\n",
    "# main_model.eval()\n",
    "# x_makeshift = []\n",
    "# y_makeshift = []\n",
    "\n",
    "# for i in range(number_of_samples):\n",
    "#     makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "#     makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "#     x_makeshift.append(makeshift_dset_features)\n",
    "#     y_makeshift.append(makeshift_dset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**functions for changing architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "i'm assuming that layers_before_train are all regular Linear layers, while layers_to_train and layers_after_train are all single layer models.\n",
    "so i'll apply relu after the layers_before_train. \n",
    "'''\n",
    "def train_some_layers(layers_before_train, layers_to_train, layers_after_train, party_index, x_makeshift, y_makeshift):\n",
    "    \n",
    "    params_to_train = []\n",
    "    \n",
    "    for layer in layers_before_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    for layer in layers_to_train:\n",
    "        layer.train()\n",
    "        params_to_train += list(layer.parameters())\n",
    "        \n",
    "    for layer in layers_after_train:\n",
    "        layer.eval()\n",
    "        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(params_to_train, lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    train_ds = TensorDataset(x_makeshift[party_index], y_makeshift[party_index])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for data, target in train_dl:\n",
    "        output = data\n",
    "        \n",
    "        for layer in layers_before_train:\n",
    "            output = layer(output)\n",
    "            output = F.relu(output)\n",
    "        for layer in layers_to_train:\n",
    "            output = layer(output)\n",
    "        for layer in layers_after_train:\n",
    "            output = layer(output)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**change architecture (just as a test run)** \n",
    "\n",
    "these are proofs-of-concept to verify that our customization process works. we won't be using the customized models, but we're making customized models anyway to show that, yeah, my methodology does its job well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# each layer is a new model.\n",
    "\n",
    "# original was        784 -> 200 -> 200 -> 10\n",
    "# p1 change to        784 -> 150 -> 100 -> 10\n",
    "\n",
    "# this will show that we can remove neurons.\n",
    "# '''\n",
    "\n",
    "# class Nn_p1_fc1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc1, self).__init__()\n",
    "#         self.fc1=nn.Linear(784,150)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc1(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc2a(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc2a, self).__init__()\n",
    "#         self.fc2=nn.Linear(200,100)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc2(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc2b(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc2b, self).__init__()\n",
    "#         self.fc2=nn.Linear(150,100)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc2(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p1_fc3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p1_fc3, self).__init__()\n",
    "#         self.fc3=nn.Linear(100,10)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x= self.fc3(x)\n",
    "#         return x\n",
    "    \n",
    "# p1_fc1 = Nn_p1_fc1()\n",
    "# p1_fc2a = Nn_p1_fc2a()\n",
    "# p1_fc2b = Nn_p1_fc2b()\n",
    "# p1_fc3 = Nn_p1_fc3()\n",
    "\n",
    "# p1_fc1.to(device)\n",
    "# p1_fc2a.to(device)\n",
    "# p1_fc2b.to(device)\n",
    "# p1_fc3.to(device)\n",
    "\n",
    "# # train fc2a (200->100), fc3 (100->10)\n",
    "# # after this the architecture will be 784 -> 200 -> 100 -> 10\n",
    "# train_some_layers(\n",
    "#     [main_model.fc1],\n",
    "#     [p1_fc2a, p1_fc3],\n",
    "#     [],\n",
    "#     0,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# # train fc1 (784->400), fc2b (400->100)\n",
    "# # after this the architecture will be 784 -> 150 -> 100 -> 10\n",
    "# train_some_layers(\n",
    "#     [],\n",
    "#     [p1_fc1, p1_fc2b],\n",
    "#     [p1_fc3],\n",
    "#     0,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# p1_model = nn.Sequential(p1_fc1,p1_fc2b,p1_fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# each layer is a new model.\n",
    "\n",
    "# original was        784 -> 200 -> 200 -> 10\n",
    "# p2 change to        784 -> 200 -> 10\n",
    "\n",
    "# this will show that we can remove entire layers\n",
    "# '''\n",
    "\n",
    "# class Nn_p2_fc1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p2_fc1, self).__init__()\n",
    "#         self.fc1=nn.Linear(784,200)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x=F.relu(self.fc1(x))\n",
    "#         return x\n",
    "    \n",
    "# class Nn_p2_fc2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Nn_p2_fc2, self).__init__()\n",
    "#         self.fc2=nn.Linear(200,10)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         return x\n",
    "    \n",
    "# p2_fc1 = Nn_p2_fc1()\n",
    "# p2_fc2 = Nn_p2_fc2()\n",
    "# p2_fc1.to(device)\n",
    "# p2_fc2.to(device)\n",
    "\n",
    "# train_some_layers(\n",
    "#     [],\n",
    "#     [p2_fc1, p2_fc2],\n",
    "#     [],\n",
    "#     1,\n",
    "#     x_makeshift,\n",
    "#     y_makeshift\n",
    "# )\n",
    "\n",
    "# p2_model = nn.Sequential(p2_fc1, p2_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test the customized models against the central post-fusion model, on the parties' own test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main_model.eval()\n",
    "# p1_model.eval()\n",
    "# p2_model.eval()\n",
    "\n",
    "# p1_test_ds = TensorDataset(x_test_dict['x_test0'], y_test_dict['y_test0'])\n",
    "# p1_test_dl = DataLoader(p1_test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "# p2_test_ds = TensorDataset(x_test_dict['x_test1'], y_test_dict['y_test1'])\n",
    "# p2_test_dl = DataLoader(p2_test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "# # (main_model_loss, main_model_acc), (cust_model_loss, cust_model_acc)\n",
    "# print('p1', validation(main_model, p1_test_dl, centralized_criterion), validation(p1_model, p1_test_dl, centralized_criterion))\n",
    "# print('p2', validation(main_model, p2_test_dl, centralized_criterion), validation(p2_model, p2_test_dl, centralized_criterion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color:#F087F9\"> Shapley </span>\n",
    "\n",
    "**calculate value of a set of parties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent confounding, we will make all parties use the same post-customization architecture.\n",
    "\n",
    "class Nn_fc1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc1, self).__init__()\n",
    "        self.fc1=nn.Linear(784,150)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc2a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc2a, self).__init__()\n",
    "        self.fc2=nn.Linear(200,100)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc2b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc2b, self).__init__()\n",
    "        self.fc2=nn.Linear(150,100)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "class Nn_fc3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Nn_fc3, self).__init__()\n",
    "        self.fc3=nn.Linear(100,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x= self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: list of parties. output: performance of the parties' models on their own datasets.\n",
    "\n",
    "def perform_whole_fedavg_and_customization_for_these_parties(parties): \n",
    "    main_model = Net2nn()\n",
    "    main_model.to(device)\n",
    "    main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "    for i in range(10):\n",
    "        model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "        start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "                                                                    y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                    y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                    name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "        main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "        # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "        # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "    gmms = [None, None, None]\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        gmms[i] = get_gmm(x_train_dict[name_of_x_train_sets[i]])\n",
    "\n",
    "    # remember each party has their own makeshift dataset.\n",
    "    x_makeshift = [None, None, None]\n",
    "    y_makeshift = [None, None, None]\n",
    "    main_model.eval()\n",
    "    \n",
    "    rv = [None, None, None]\n",
    "\n",
    "    for i in [0,1,2]:\n",
    "        makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "        makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "        x_makeshift[i] = makeshift_dset_features\n",
    "        y_makeshift[i] = makeshift_dset_labels\n",
    "\n",
    "        nn_fc1 = Nn_fc1()\n",
    "        nn_fc2a = Nn_fc2a()\n",
    "        nn_fc2b = Nn_fc2b()\n",
    "        nn_fc3 = Nn_fc3()\n",
    "        \n",
    "        nn_fc1.to(device)\n",
    "        nn_fc2a.to(device)\n",
    "        nn_fc2b.to(device)\n",
    "        nn_fc3.to(device)\n",
    "\n",
    "        train_some_layers(\n",
    "            [main_model.fc1],\n",
    "            [nn_fc2a, nn_fc3],\n",
    "            [],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        train_some_layers(\n",
    "            [],\n",
    "            [nn_fc1, nn_fc2b],\n",
    "            [nn_fc3],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "        customized_model.eval()\n",
    "        \n",
    "        # TODO testing should be done with makeshift dataset, not the real test set. fix it when/if you have the time. not a priority.\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "        validation_results = validation(customized_model, test_dl, centralized_criterion)\n",
    "        rv[i] = validation_results[1]\n",
    "        \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test to make sure it runs with no error\n",
    "# [perform_whole_fedavg_and_customization_for_these_parties([0,1,2]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([0]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([2]),\n",
    "#  perform_whole_fedavg_and_customization_for_these_parties([1,2])\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**actual shapley calculation stuff - diff label distribs (exp 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def my_sum(xs):\n",
    "#     rv = 0\n",
    "#     for x in xs:\n",
    "#         if x != None:\n",
    "#             rv += x\n",
    "#     return rv\n",
    "        \n",
    "# the_other_parties = [[1,2],[0,2],[0,1]]\n",
    "\n",
    "# grand_coalition_value = my_sum(perform_whole_fedavg_and_customization_for_these_parties([0,1,2]))\n",
    "# single_party_values = [\n",
    "#     my_sum(perform_whole_fedavg_and_customization_for_these_parties([party])) for party in [0,1,2]]\n",
    "# two_party_values_indexed_by_missing_party = [\n",
    "#     my_sum(perform_whole_fedavg_and_customization_for_these_parties(the_other_parties[party])) for party in [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iters_to_avg_over = 1 if DEBUGGING_RUN_FAST else 10\n",
    "# total_shapley_tus = [0,0,0]\n",
    "\n",
    "# for j in range(iters_to_avg_over):\n",
    "#     for i in range(3): # let's say we wanna find shapley of party 0.\n",
    "#         total_marginal_contribution = 2 * single_party_values[i] # v([0]) - v([]). accounts for 012 and 021 permutations\n",
    "#         other_party_a, other_party_b = the_other_parties[i]\n",
    "#         total_marginal_contribution += (\n",
    "#             two_party_values_indexed_by_missing_party[other_party_a] - single_party_values[other_party_b]) # v([0,1]) - v([1]). 102 permutation\n",
    "#         total_marginal_contribution += (\n",
    "#             two_party_values_indexed_by_missing_party[other_party_b] - single_party_values[other_party_a]) # v([0,2]) - v([2]). 201 permutation\n",
    "#         total_marginal_contribution += 2 * (\n",
    "#             grand_coalition_value - two_party_values_indexed_by_missing_party[i]) # v([0,1,2]) - v([1,2]). 210, 120 permutations\n",
    "#         total_shapley_tus[i] += total_marginal_contribution / 6\n",
    "\n",
    "# shapley_tu = [s/iters_to_avg_over for s in total_shapley_tus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5405056446821153, 0.8666339869281044, 1.0323648247177657]\n"
     ]
    }
   ],
   "source": [
    "# # these are actly the shapley ntu values too.\n",
    "# print(shapley_tu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(['0','1','2'], shapley_tu)\n",
    "# plt.suptitle('Graph of Shapley-NTU contribution against party index')\n",
    "# plt.xlabel('Party index')\n",
    "# plt.ylabel('Shapley-NTU contribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**swapping one layer at a time beats retraining the whole model (exp 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "same as the earlier method except we don't swap layers. we retrain a whole new model from scratch.\n",
    "'''\n",
    "def perform_whole_fedavg_and_customization_for_these_parties_retrain_whole_model(parties): \n",
    "    main_model = Net2nn()\n",
    "    main_model.to(device)\n",
    "    main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    main_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_these_parties(parties)\n",
    "\n",
    "    for i in range(10):\n",
    "        model_dict=send_main_model_to_nodes_and_update_model_dict_for_these_parties(main_model, model_dict, parties, name_of_models)\n",
    "        start_train_end_node_process_print_some_for_these_parties(parties, -1, x_train_dict, name_of_x_train_sets,\n",
    "                                                                    y_train_dict, name_of_y_train_sets, x_test_dict, name_of_x_test_sets,\n",
    "                                                                    y_test_dict, name_of_y_test_sets, model_dict, name_of_models, criterion_dict,\n",
    "                                                                    name_of_criterions, optimizer_dict, name_of_optimizers)\n",
    "        main_model= set_averaged_weights_as_main_model_weights_and_update_main_model_for_these_parties(main_model,model_dict, parties, name_of_models) \n",
    "        \n",
    "        # test_loss, test_accuracy = validation(main_model, test_dl, main_criterion)\n",
    "        # print(\"Iteration\", str(i+2), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))  \n",
    "   \n",
    "    gmms = [None, None, None]\n",
    "    \n",
    "    for i in [0,1,2]:\n",
    "        gmms[i] = get_gmm(x_train_dict[name_of_x_train_sets[i]])\n",
    "\n",
    "    # remember each party has their own makeshift dataset.\n",
    "    x_makeshift = [None, None, None]\n",
    "    y_makeshift = [None, None, None]\n",
    "    main_model.eval()\n",
    "    \n",
    "    rv = [None, None, None]\n",
    "\n",
    "    for i in [0,1,2]:\n",
    "        makeshift_dset_features = gmm_generate_data(gmms[i], MAKESHIFT_DSET_SIZE)\n",
    "        makeshift_dset_labels = main_model(makeshift_dset_features).argmax(dim=1)\n",
    "        x_makeshift[i] = makeshift_dset_features\n",
    "        y_makeshift[i] = makeshift_dset_labels\n",
    "\n",
    "        nn_fc1 = Nn_fc1()\n",
    "        nn_fc2b = Nn_fc2b()\n",
    "        nn_fc3 = Nn_fc3()\n",
    "        \n",
    "        nn_fc1.to(device)\n",
    "        nn_fc2b.to(device)\n",
    "        nn_fc3.to(device)\n",
    "\n",
    "        train_some_layers(\n",
    "            [],\n",
    "            [nn_fc1, nn_fc2b, nn_fc3],\n",
    "            [],\n",
    "            i, x_makeshift, y_makeshift\n",
    "        )\n",
    "\n",
    "        customized_model = nn.Sequential(nn_fc1,nn_fc2b,nn_fc3)\n",
    "        customized_model.eval()\n",
    "        \n",
    "        # TODO testing should be done with makeshift dataset, not the real test set. fix it when/if you have the time. not a priority.\n",
    "        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n",
    "        test_dl = DataLoader(test_ds, batch_size= batch_size * 2)\n",
    "\n",
    "        validation_results = validation(customized_model, test_dl, centralized_criterion)\n",
    "        rv[i] = validation_results[1]\n",
    "        \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 "
     ]
    }
   ],
   "source": [
    "train_layer_by_layer_val_results = []\n",
    "retrain_whole_model_val_results = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(i,end=' ')\n",
    "    train_layer_by_layer_val_results.append(perform_whole_fedavg_and_customization_for_these_parties([0,1,2]))\n",
    "    retrain_whole_model_val_results.append(perform_whole_fedavg_and_customization_for_these_parties_retrain_whole_model([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8113333333333334, 0.88, 0.8873333333333333], [0.8433333333333334, 0.8793333333333333, 0.8733333333333333], [0.8266666666666667, 0.8813333333333333, 0.8813333333333333], [0.832, 0.8786666666666667, 0.874], [0.8273333333333334, 0.8666666666666667, 0.882], [0.8333333333333334, 0.8893333333333333, 0.874], [0.8426666666666667, 0.8613333333333333, 0.8773333333333333], [0.8426666666666667, 0.8786666666666667, 0.8766666666666667], [0.8306666666666667, 0.8713333333333333, 0.8826666666666667], [0.8286666666666667, 0.8713333333333333, 0.87], [0.838, 0.8466666666666667, 0.88], [0.8446666666666667, 0.8606666666666667, 0.8813333333333333], [0.8306666666666667, 0.89, 0.8726666666666667], [0.8313333333333334, 0.8773333333333333, 0.8866666666666667], [0.848, 0.874, 0.8933333333333333], [0.846, 0.878, 0.8886666666666667], [0.8373333333333334, 0.8766666666666667, 0.87], [0.838, 0.8866666666666667, 0.882], [0.8406666666666667, 0.8746666666666667, 0.878], [0.842, 0.8793333333333333, 0.8793333333333333]]\n",
      "[0.782, 0.8586666666666667, 0.852]\n"
     ]
    }
   ],
   "source": [
    "print(train_layer_by_layer_val_results)\n",
    "print(retrain_whole_model_val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([\n",
    "    list(map(lambda x: x[0], train_layer_by_layer_val_results)),\n",
    "    list(map(lambda x: x[0], retrain_whole_model_val_results)),\n",
    "    list(map(lambda x: x[1], train_layer_by_layer_val_results)),\n",
    "    list(map(lambda x: x[1], retrain_whole_model_val_results)),\n",
    "    list(map(lambda x: x[2], train_layer_by_layer_val_results)),\n",
    "    list(map(lambda x: x[2], retrain_whole_model_val_results)),\n",
    "])\n",
    "\n",
    "plt.xticks(range(1,7), ['0L','0M','1L','1M','2L','2M'])\n",
    "plt.suptitle('Validation accuracies with different training methods')\n",
    "plt.xlabel('Party index and training method (L=layer, M=model)')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
